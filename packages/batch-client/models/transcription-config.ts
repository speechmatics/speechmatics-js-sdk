/* tslint:disable */
/* eslint-disable */
/**
 * Speechmatics ASR REST API
 * The Speechmatics Automatic Speech Recognition REST API is used to submit ASR jobs and receive the results. The supported job type is transcription of audio files.
 *
 * The version of the OpenAPI document: 2.0.0
 * Contact: support@speechmatics.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

// May contain unused imports in some cases
// @ts-ignore
import type { OperatingPoint } from './operating-point';
// May contain unused imports in some cases
// @ts-ignore
import type { TranscriptionConfigAdditionalVocabInner } from './transcription-config-additional-vocab-inner';
// May contain unused imports in some cases
// @ts-ignore
import type { TranscriptionConfigAudioFilteringConfig } from './transcription-config-audio-filtering-config';
// May contain unused imports in some cases
// @ts-ignore
import type { TranscriptionConfigPunctuationOverrides } from './transcription-config-punctuation-overrides';
// May contain unused imports in some cases
// @ts-ignore
import type { TranscriptionConfigSpeakerDiarizationConfig } from './transcription-config-speaker-diarization-config';

/**
 *
 * @export
 * @interface TranscriptionConfig
 */
export interface TranscriptionConfig {
  /**
   * Language model to process the audio input, normally specified as an ISO language code
   * @type {string}
   * @memberof TranscriptionConfig
   */
  language: string;
  /**
   * Request a specialized model based on \'language\' but optimized for a particular field, e.g. \"finance\" or \"medical\".
   * @type {string}
   * @memberof TranscriptionConfig
   */
  domain?: string;
  /**
   * Language locale to be used when generating the transcription output, normally specified as an ISO language code
   * @type {string}
   * @memberof TranscriptionConfig
   */
  output_locale?: string;
  /**
   *
   * @type {OperatingPoint}
   * @memberof TranscriptionConfig
   */
  operating_point?: OperatingPoint;
  /**
   * List of custom words or phrases that should be recognized. Alternative pronunciations can be specified to aid recognition.
   * @type {Array<TranscriptionConfigAdditionalVocabInner>}
   * @memberof TranscriptionConfig
   */
  additional_vocab?: Array<TranscriptionConfigAdditionalVocabInner>;
  /**
   *
   * @type {TranscriptionConfigPunctuationOverrides}
   * @memberof TranscriptionConfig
   */
  punctuation_overrides?: TranscriptionConfigPunctuationOverrides;
  /**
   * Specify whether speaker or channel labels are added to the transcript. The default is `none`.   - **none**: no speaker or channel labels are added.   - **speaker**: speaker attribution is performed based on acoustic matching;              all input channels are mixed into a single stream for processing.   - **channel**: multiple input channels are processed individually and collated             into a single transcript.   - **speaker_change**: the output indicates when the speaker in the audio changes.                     No speaker attribution is performed. This is a faster method                     than speaker. The reported speaker changes may not agree with speaker.   - **channel_and_speaker_change**: both channel and speaker_change are switched on.                                 The speaker change is indicated if more than one speaker                                 are recorded in one channel.
   * @type {string}
   * @memberof TranscriptionConfig
   */
  diarization?: TranscriptionConfigDiarizationEnum;
  /**
   * Ranges between zero and one. Controls how responsive the system is for potential speaker changes. High value indicates high sensitivity. Defaults to 0.4.
   * @type {number}
   * @memberof TranscriptionConfig
   */
  speaker_change_sensitivity?: number;
  /**
   * Transcript labels to use when using collating separate input channels.
   * @type {Array<string>}
   * @memberof TranscriptionConfig
   */
  channel_diarization_labels?: Array<string>;
  /**
   * Include additional \'entity\' objects in the transcription results (e.g. dates, numbers) and their original spoken form. These entities are interleaved with other types of results. The concatenation of these words is represented as a single entity with the concatenated written form present in the \'content\' field. The entities contain a \'spoken_form\' field, which can be used in place of the corresponding \'word\' type results, in case a spoken form is preferred to a written form. They also contain a \'written_form\', which can be used instead of the entity, if you want a breakdown of the words without spaces. They can still contain non-breaking spaces and other special whitespace characters, as they are considered part of the word for the formatting output. In case of a written_form, the individual word times are estimated and might not be accurate if the order of the words in the written form does not correspond to the order they were actually spoken (such as \'one hundred million dollars\' and \'$100 million\').
   * @type {boolean}
   * @memberof TranscriptionConfig
   */
  enable_entities?: boolean;
  /**
   * Whether or not to enable flexible endpointing and allow the entity to continue to be spoken.
   * @type {string}
   * @memberof TranscriptionConfig
   */
  max_delay_mode?: TranscriptionConfigMaxDelayModeEnum;
  /**
   *
   * @type {TranscriptionConfigSpeakerDiarizationConfig}
   * @memberof TranscriptionConfig
   */
  speaker_diarization_config?: TranscriptionConfigSpeakerDiarizationConfig;
  /**
   *
   * @type {TranscriptionConfigAudioFilteringConfig}
   * @memberof TranscriptionConfig
   */
  audio_filtering_config?: TranscriptionConfigAudioFilteringConfig;
}

export const TranscriptionConfigDiarizationEnum = {
  None: 'none',
  Speaker: 'speaker',
  Channel: 'channel',
  SpeakerChange: 'speaker_change',
  ChannelAndSpeakerChange: 'channel_and_speaker_change',
} as const;

export type TranscriptionConfigDiarizationEnum =
  (typeof TranscriptionConfigDiarizationEnum)[keyof typeof TranscriptionConfigDiarizationEnum];
export const TranscriptionConfigMaxDelayModeEnum = {
  Fixed: 'fixed',
  Flexible: 'flexible',
} as const;

export type TranscriptionConfigMaxDelayModeEnum =
  (typeof TranscriptionConfigMaxDelayModeEnum)[keyof typeof TranscriptionConfigMaxDelayModeEnum];
